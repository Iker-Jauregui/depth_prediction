{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d459c600-19d7-42a1-bf03-75ed95eb9483",
   "metadata": {},
   "source": [
    "# 1. Notebook Introduction\n",
    "This is a utility notebook that performs the following operations:\n",
    "- First, it loads one of the biggest Depth Anything V3 model (DA3NESTED-GIANT-LARGE) and feeds the model with one random image in order to assure that the model works correctly.\n",
    "- After that, the notebook loads the original CamVid dataset and generates the necessary new data used on this project (depth maps, confidence maps, segmentation masks...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff62af-190c-4a26-a966-0f0b88e523f2",
   "metadata": {},
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c3cc3-4b4c-4e3b-9981-86449a9df79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from depth_anything_3.api import DepthAnything3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0375697-9263-48b0-ba1d-262713b35a8f",
   "metadata": {},
   "source": [
    "# 3. Loading model\n",
    "Download and creation of the Depth Anything V3 model.\n",
    "Caution, it is almost 7GB!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c81eee-0180-4c95-9cf4-d707b4bda9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from Hugging Face Hub\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DepthAnything3.from_pretrained(\"depth-anything/DA3NESTED-GIANT-LARGE\")\n",
    "model = model.to(device=device)\n",
    "model.eval()\n",
    "print('Model loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682925a4-9cee-4d8f-a0db-afca959d5776",
   "metadata": {},
   "source": [
    "# 4. Toy Example\n",
    "This is just a toy example to assure that the prediction of the model works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f276a0d-4dfe-4ba1-ba66-501f1e9ba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_image = ['/home/alumno/Desktop/datos/Computer Vision/depth-anything-3/CamVid/train/0001TP_009210.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855532d9-b708-4481-9501-869ddccf116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_pred = model.inference(\n",
    "    toy_image,\n",
    "    process_res=960\n",
    ")\n",
    "print(toy_pred.depth[0].shape)\n",
    "print(toy_pred.conf[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018d307-d17e-4228-b13d-8502d7f36d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(toy_pred.depth[0]))\n",
    "print(type(toy_pred.conf[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e362d-24e8-4e4f-917c-48f4ef7c0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803477d-faef-4961-9ec2-3a21c3faf6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy array to tensor\n",
    "depth_tensor = torch.from_numpy(toy_pred.depth[0])\n",
    "\n",
    "# Add batch and channel dimensions: (H, W) -> (1, 1, H, W)\n",
    "depth_tensor = depth_tensor.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Resize to 720x960\n",
    "resized = TF.resize(depth_tensor, size=[720, 960], antialias=True)\n",
    "\n",
    "# Remove batch and channel dimensions: (1, 1, H, W) -> (H, W)\n",
    "resized = resized.squeeze(0).squeeze(0)\n",
    "\n",
    "# Convert back to numpy if needed\n",
    "resized_numpy = resized.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe93f4-0c81-41fd-b9c8-ac1ff0329f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(resized_numpy, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6aef7-0012-4ce0-bf30-8a953aa41918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .npy file (preserves exact float values)\n",
    "np.save('depth_map.npy', resized_numpy)\n",
    "\n",
    "# Load it back later\n",
    "loaded = np.load('depth_map.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfac677-0d77-403a-a67b-5837d4be9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(loaded, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6ae1a-de0c-496e-984a-98a4aecb6917",
   "metadata": {},
   "source": [
    "# 5. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76c322-0fa3-4193-aedf-fae0989c64b3",
   "metadata": {},
   "source": [
    "## 5.1 Depth maps generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9add9d-2798-42d0-8525-1a5f9f12c375",
   "metadata": {},
   "source": [
    "### Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3216f3-bad2-45a6-95bd-f73664d2d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../CamVid/train/'\n",
    "OUTPUT_DIR = '../CamVid/train_labels/'\n",
    "DEPTH_MAP_DIR = 'train_depths/'\n",
    "CONFS_DIR = 'train_confs/'\n",
    "IMAGES = os.listdir(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa1219-5125-420b-96ab-6e19569b4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in tqdm(IMAGES):\n",
    "    pred = model.inference(\n",
    "        [INPUT_DIR + img],\n",
    "        process_res=960\n",
    "    )\n",
    "\n",
    "    # Convert numpy array to tensor\n",
    "    depth_tensor = torch.from_numpy(pred.depth[0])\n",
    "    \n",
    "    # Add batch and channel dimensions: (H, W) -> (1, 1, H, W)\n",
    "    depth_tensor = depth_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Resize to 720x960\n",
    "    resized = TF.resize(depth_tensor, size=[720, 960], antialias=True)\n",
    "    \n",
    "    # Remove batch and channel dimensions: (1, 1, H, W) -> (H, W)\n",
    "    resized = resized.squeeze(0).squeeze(0)\n",
    "    \n",
    "    # Convert back to numpy if needed\n",
    "    resized_numpy = resized.numpy()\n",
    "\n",
    "    np.save(OUTPUT_DIR + DEPTH_MAP_DIR + img.replace('png', 'npy'), resized_numpy)\n",
    "\n",
    "    conf_tensor = torch.from_numpy(pred.conf[0])\n",
    "    conf_tensor = conf_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    resized_conf = TF.resize(conf_tensor, size=[720, 960], antialias=True)\n",
    "    resized_conf_numpy = resized_conf.squeeze(0).squeeze(0).numpy()\n",
    "    \n",
    "    # Save resized confidence\n",
    "    np.save(OUTPUT_DIR + CONFS_DIR + img.replace('png', 'npy'), resized_conf_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072d411-8543-431e-a5bd-5ef53ef205d5",
   "metadata": {},
   "source": [
    "### Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9897dcc-8f88-4151-adf2-b6e1eb227710",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../CamVid/val/'\n",
    "OUTPUT_DIR = '../CamVid/val_labels/'\n",
    "DEPTH_MAP_DIR = 'val_depths/'\n",
    "CONFS_DIR = 'val_confs/'\n",
    "IMAGES = os.listdir(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdae6e-d87e-4ce8-ab9e-059c688966a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in tqdm(IMAGES):\n",
    "    pred = model.inference(\n",
    "        [INPUT_DIR + img],\n",
    "        process_res=960\n",
    "    )\n",
    "\n",
    "    # Convert numpy array to tensor\n",
    "    depth_tensor = torch.from_numpy(pred.depth[0])\n",
    "    \n",
    "    # Add batch and channel dimensions: (H, W) -> (1, 1, H, W)\n",
    "    depth_tensor = depth_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Resize to 720x960\n",
    "    resized = TF.resize(depth_tensor, size=[720, 960], antialias=True)\n",
    "    \n",
    "    # Remove batch and channel dimensions: (1, 1, H, W) -> (H, W)\n",
    "    resized = resized.squeeze(0).squeeze(0)\n",
    "    \n",
    "    # Convert back to numpy if needed\n",
    "    resized_numpy = resized.numpy()\n",
    "\n",
    "    np.save(OUTPUT_DIR + DEPTH_MAP_DIR + img.replace('png', 'npy'), resized_numpy)\n",
    "    \n",
    "    conf_tensor = torch.from_numpy(pred.conf[0])\n",
    "    conf_tensor = conf_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    resized_conf = TF.resize(conf_tensor, size=[720, 960], antialias=True)\n",
    "    resized_conf_numpy = resized_conf.squeeze(0).squeeze(0).numpy()\n",
    "    \n",
    "    # Save resized confidence\n",
    "    np.save(OUTPUT_DIR + CONFS_DIR + img.replace('png', 'npy'), resized_conf_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976e2ad-4fdf-4067-a340-7bc42df91213",
   "metadata": {},
   "source": [
    "### Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08a427-be57-4ea2-945a-850ce2ff8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../CamVid/test/'\n",
    "OUTPUT_DIR = '../CamVid/test_labels/'\n",
    "DEPTH_MAP_DIR = 'test_depths/'\n",
    "CONFS_DIR = 'test_confs/'\n",
    "IMAGES = os.listdir(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ceabb-153a-4c14-b149-6716b766831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in tqdm(IMAGES):\n",
    "    pred = model.inference(\n",
    "        [INPUT_DIR + img],\n",
    "        process_res=960\n",
    "    )\n",
    "\n",
    "    # Convert numpy array to tensor\n",
    "    depth_tensor = torch.from_numpy(pred.depth[0])\n",
    "    \n",
    "    # Add batch and channel dimensions: (H, W) -> (1, 1, H, W)\n",
    "    depth_tensor = depth_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Resize to 720x960\n",
    "    resized = TF.resize(depth_tensor, size=[720, 960], antialias=True)\n",
    "    \n",
    "    # Remove batch and channel dimensions: (1, 1, H, W) -> (H, W)\n",
    "    resized = resized.squeeze(0).squeeze(0)\n",
    "    \n",
    "    # Convert back to numpy if needed\n",
    "    resized_numpy = resized.numpy()\n",
    "\n",
    "    np.save(OUTPUT_DIR + DEPTH_MAP_DIR + img.replace('png', 'npy'), resized_numpy)\n",
    "    \n",
    "    conf_tensor = torch.from_numpy(pred.conf[0])\n",
    "    conf_tensor = conf_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    resized_conf = TF.resize(conf_tensor, size=[720, 960], antialias=True)\n",
    "    resized_conf_numpy = resized_conf.squeeze(0).squeeze(0).numpy()\n",
    "    \n",
    "    # Save resized confidence\n",
    "    np.save(OUTPUT_DIR + CONFS_DIR + img.replace('png', 'npy'), resized_conf_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768cfb2-fce9-4e27-b9fb-bbeb191ece3b",
   "metadata": {},
   "source": [
    "## 5.2 Save Segmentation Masks as NumPy Arrays\n",
    "Using precomputed segmentation masks will speed up MultiTaskUnet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6b479-b3d0-4319-8c23-ca7b4ad9901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationLUT:\n",
    "    \"\"\"\n",
    "    Ultra-fast RGB to class_id conversion using 3D Lookup Table.\n",
    "    Creates a 256x256x256 LUT that maps any RGB value directly to class_id.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_dict_path):\n",
    "        # Load class dictionary\n",
    "        self.rgb_to_class_id = {}\n",
    "        self.class_id_to_name = {}\n",
    "        \n",
    "        with open(class_dict_path) as f:\n",
    "            next(f)  # Skip header\n",
    "            for idx, line in enumerate(f):\n",
    "                name, r, g, b = line.strip().split(\",\")\n",
    "                self.rgb_to_class_id[(int(r), int(g), int(b))] = idx\n",
    "                self.class_id_to_name[idx] = name\n",
    "        \n",
    "        self.num_classes = len(self.rgb_to_class_id)\n",
    "        \n",
    "        # Create 3D LUT: 256x256x256 -> class_id\n",
    "        # Default to 0 (background) or -1 for unknown colors\n",
    "        print(\"Creating 3D Lookup Table for segmentation masks...\")\n",
    "        self.lut = np.zeros((256, 256, 256), dtype=np.int64)\n",
    "        \n",
    "        # Fill LUT with class mappings\n",
    "        for (r, g, b), class_id in self.rgb_to_class_id.items():\n",
    "            self.lut[r, g, b] = class_id\n",
    "        \n",
    "        print(f\"LUT created. {len(self.rgb_to_class_id)} classes mapped.\")\n",
    "    \n",
    "    def rgb_to_labels(self, seg_rgb):\n",
    "        \"\"\"\n",
    "        Convert RGB segmentation mask to class labels using LUT.\n",
    "        \n",
    "        Args:\n",
    "            seg_rgb: numpy array of shape (H, W, 3) with uint8 RGB values\n",
    "            \n",
    "        Returns:\n",
    "            labels: numpy array of shape (H, W) with int64 class IDs\n",
    "        \"\"\"\n",
    "        # Direct indexing into LUT - O(H*W) operation, extremely fast\n",
    "        return self.lut[seg_rgb[:, :, 0], seg_rgb[:, :, 1], seg_rgb[:, :, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed9667-7a13-4b5f-9cd7-5630f39911c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_segmentation_masks(seg_dir, output_dir, class_dict_path):\n",
    "    \"\"\"\n",
    "    Pre-convert all RGB segmentation masks to class ID numpy arrays.\n",
    "    Run this ONCE before training.\n",
    "    \"\"\"    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create LUT\n",
    "    lut = SegmentationLUT(class_dict_path)\n",
    "    \n",
    "    seg_files = sorted([f for f in os.listdir(seg_dir) if f.endswith('.png')])\n",
    "    \n",
    "    print(f\"Pre-processing {len(seg_files)} segmentation masks...\")\n",
    "    for filename in tqdm(seg_files):\n",
    "        # Load RGB mask\n",
    "        seg_path = os.path.join(seg_dir, filename)\n",
    "        seg_rgb = np.array(Image.open(seg_path).convert('RGB'), dtype=np.uint8)\n",
    "        \n",
    "        # Convert to class IDs\n",
    "        seg_labels = lut.rgb_to_labels(seg_rgb)\n",
    "        \n",
    "        # Save as numpy array\n",
    "        output_path = os.path.join(output_dir, filename.replace('.png', '.npy'))\n",
    "        np.save(output_path, seg_labels.astype(np.int16))  # int16 saves space\n",
    "    \n",
    "    print(f\"Saved preprocessed masks to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93abe8d-b6af-4320-a015-8615603f20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SEG_DIR = '../CamVid/train_labels/train_seg/'\n",
    "VAL_SEG_DIR = '../CamVid/val_labels/val_seg/'\n",
    "TEST_SEG_DIR = '../CamVid/test_labels/test_seg/'\n",
    "CLASS_DICT_PATH = '../CamVid/class_dict.csv'\n",
    "\n",
    "preprocess_segmentation_masks(TRAIN_SEG_DIR, '../CamVid/train_labels/train_seg_npy/', CLASS_DICT_PATH)\n",
    "preprocess_segmentation_masks(VAL_SEG_DIR, '../CamVid/val_labels/val_seg_npy/', CLASS_DICT_PATH)\n",
    "preprocess_segmentation_masks(TEST_SEG_DIR, '../CamVid/test_labels/test_seg_npy/', CLASS_DICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356e1b1-6ceb-469b-87b9-5ec0758ea34c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
