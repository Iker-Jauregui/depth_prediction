{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890800b3-49cb-415b-86ba-fce6f22e1019",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ea1fd-e8d7-493b-9fa8-f85f14100e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from collections import defaultdict\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import optuna\n",
    "import optuna.visualization as viz\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b372cb9-5943-497f-92f8-f6a40be48445",
   "metadata": {},
   "source": [
    "# Classes and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce03c7-d877-4f3b-bc0a-fef521902d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDistillationDataset(Dataset):\n",
    "    \"\"\"Optimized dataset that loads pre-processed numpy masks.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, depth_dir, seg_dir, num_classes, is_train=False, prob=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.seg_dir = seg_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.is_train = is_train\n",
    "        self.prob = prob if prob is not None else {'horizontal_flip': 0.5, 'color_jitter': 0.5}\n",
    "        \n",
    "        self.images = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        base_name = img_name.replace('.png', '')\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n",
    "        \n",
    "        # Load depth\n",
    "        teacher_depth = np.load(os.path.join(self.depth_dir, f\"{base_name}.npy\"))\n",
    "\n",
    "        # Load pre-processed segmentation mask (already class IDs!)\n",
    "        seg_mask = np.load(os.path.join(self.seg_dir, f\"{base_name}.npy\"))\n",
    "        \n",
    "        # Augmentations\n",
    "        if self.is_train:\n",
    "            if torch.rand(1).item() < self.prob['horizontal_flip']:\n",
    "                image = TF.hflip(image)\n",
    "                teacher_depth = np.fliplr(teacher_depth).copy()\n",
    "                seg_mask = np.fliplr(seg_mask).copy()\n",
    "            \n",
    "            if torch.rand(1).item() < self.prob['color_jitter']:\n",
    "                image = T.ColorJitter(brightness=0.25, contrast=0.25)(image)\n",
    "        \n",
    "        # To tensors\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        teacher_depth = torch.from_numpy(teacher_depth).float()\n",
    "        seg_mask = torch.from_numpy(seg_mask.copy()).long()\n",
    "        \n",
    "        return image, teacher_depth, seg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe3249-b1d6-4914-adb5-6dcf65c5df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 train_img_dir, train_depth_dir, train_seg_dir,\n",
    "                 val_img_dir, val_depth_dir, val_seg_dir,\n",
    "                 test_img_dir, test_depth_dir, test_seg_dir,\n",
    "                 num_classes=32,\n",
    "                 batch_size=4, \n",
    "                 num_workers=2,\n",
    "                 prob=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_img_dir, train_depth_dir, train_seg_dir: Training data directories\n",
    "            val_img_dir, val_depth_dir, val_seg_dir: Validation data directories\n",
    "            test_img_dir, test_depth_dir, test_seg_dir: Test data directories\n",
    "            batch_size: Batch size for dataloaders\n",
    "            num_workers: Number of workers for dataloaders\n",
    "            prob: Dictionary with augmentation probabilities\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.train_img_dir = train_img_dir\n",
    "        self.train_depth_dir = train_depth_dir\n",
    "        self.train_seg_dir = train_seg_dir\n",
    "        self.val_img_dir = val_img_dir\n",
    "        self.val_depth_dir = val_depth_dir\n",
    "        self.val_seg_dir = val_seg_dir\n",
    "        self.test_img_dir = test_img_dir\n",
    "        self.test_depth_dir = test_depth_dir\n",
    "        self.test_seg_dir = test_seg_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.prob = prob if prob is not None else {'horizontal_flip': 0.5, 'color_jitter': 0.5}\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = DepthDistillationDataset(\n",
    "                img_dir=self.train_img_dir,\n",
    "                depth_dir=self.train_depth_dir,\n",
    "                seg_dir=self.train_seg_dir,\n",
    "                num_classes=self.num_classes,\n",
    "                is_train=True,\n",
    "                prob=self.prob\n",
    "            )\n",
    "            self.val_dataset = DepthDistillationDataset(\n",
    "                img_dir=self.val_img_dir,\n",
    "                depth_dir=self.val_depth_dir,\n",
    "                seg_dir=self.val_seg_dir,\n",
    "                num_classes=self.num_classes,\n",
    "                is_train=False\n",
    "            )\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = DepthDistillationDataset(\n",
    "                img_dir=self.test_img_dir,\n",
    "                depth_dir=self.test_depth_dir,\n",
    "                seg_dir=self.test_seg_dir,\n",
    "                num_classes=self.num_classes,\n",
    "                is_train=False\n",
    "            )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True if self.num_workers > 0 else False\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True if self.num_workers > 0 else False\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True if self.num_workers > 0 else False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80773f-a525-4595-ac08-0ce655e75c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthSegMultiTaskLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task loss with learnable uncertainty weighting.\n",
    "    Based on \"Multi-Task Learning Using Uncertainty to Weigh Losses\" (Kendall et al., 2018)\n",
    "    \"\"\"\n",
    "    def __init__(self, init_log_var_depth=0.0, init_log_var_seg=0.0):\n",
    "        super().__init__()\n",
    "        self.log_var_depth = nn.Parameter(torch.tensor(init_log_var_depth))\n",
    "        self.log_var_seg = nn.Parameter(torch.tensor(init_log_var_seg))\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, depth_pred, depth_target, seg_pred, seg_target):\n",
    "        # Ensure correct shapes\n",
    "        if depth_pred.dim() == 4:\n",
    "            depth_pred = depth_pred.squeeze(1)\n",
    "        \n",
    "        # Task losses\n",
    "        depth_loss = self.mse_loss(depth_pred, depth_target)\n",
    "        seg_loss = self.ce_loss(seg_pred, seg_target)\n",
    "        \n",
    "        # Uncertainty weighting\n",
    "        # precision = 1/σ², using log_var = log(σ²) for stability\n",
    "        precision_depth = torch.exp(-self.log_var_depth)\n",
    "        precision_seg = torch.exp(-self.log_var_seg)\n",
    "        \n",
    "        weighted_depth = 0.5 * precision_depth * depth_loss + 0.5 * self.log_var_depth\n",
    "        weighted_seg = 0.5 * precision_seg * seg_loss + 0.5 * self.log_var_seg\n",
    "        \n",
    "        total_loss = weighted_depth + weighted_seg\n",
    "        \n",
    "        # Return individual losses for logging\n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'depth_loss': depth_loss,\n",
    "            'seg_loss': seg_loss,\n",
    "            'weight_depth': precision_depth.detach(),\n",
    "            'weight_seg': precision_seg.detach()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4857ec-9c49-4dec-8d22-78071c2bdda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthSegDistillationModule(pl.LightningModule):\n",
    "    def __init__(self, student_model, lr=3e-4, weight_decay=0.0):\n",
    "        super().__init__()\n",
    "        self.student = student_model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Multi-task loss with uncertainty weighting\n",
    "        self.loss_fn = DepthSegMultiTaskLoss()\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['student_model'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.student(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, teacher_depth, seg_mask = batch\n",
    "        \n",
    "        # Get predictions\n",
    "        depth_pred, seg_pred = self(images)\n",
    "        \n",
    "        # Calculate multi-task loss\n",
    "        loss_dict = self.loss_fn(depth_pred, teacher_depth, seg_pred, seg_mask)\n",
    "        \n",
    "        # Log all metrics\n",
    "        self.log('train_total_loss', loss_dict['total_loss'], on_step=False, on_epoch=True)\n",
    "        self.log('train_depth_loss', loss_dict['depth_loss'], on_step=False, on_epoch=True)\n",
    "        self.log('train_seg_loss', loss_dict['seg_loss'], on_step=False, on_epoch=True)\n",
    "        self.log('train_weight_depth', loss_dict['weight_depth'], on_step=False, on_epoch=True)\n",
    "        self.log('train_weight_seg', loss_dict['weight_seg'], on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss_dict['total_loss']\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, teacher_depth, seg_mask = batch\n",
    "        \n",
    "        depth_pred, seg_pred = self(images)\n",
    "        loss_dict = self.loss_fn(depth_pred, teacher_depth, seg_pred, seg_mask)\n",
    "        \n",
    "        self.log('val_total_loss', loss_dict['total_loss'], on_step=False, on_epoch=True)\n",
    "        self.log('val_depth_loss', loss_dict['depth_loss'], on_step=False, on_epoch=True)\n",
    "        self.log('val_seg_loss', loss_dict['seg_loss'], on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss_dict['total_loss']\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, teacher_depth, seg_mask = batch\n",
    "        \n",
    "        depth_pred, seg_pred = self(images)\n",
    "        loss_dict = self.loss_fn(depth_pred, teacher_depth, seg_pred, seg_mask)\n",
    "        \n",
    "        self.log('test_total_loss', loss_dict['total_loss'], on_step=False, on_epoch=True)\n",
    "        self.log('test_depth_loss', loss_dict['depth_loss'], on_step=False, on_epoch=True)\n",
    "        self.log('test_seg_loss', loss_dict['seg_loss'], on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss_dict['total_loss']\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),  # Includes loss function parameters!\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=20\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_total_loss'\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f833d-3594-4eb7-9dfc-5f971fcefa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskUnet(nn.Module):\n",
    "    def __init__(self, num_classes, encoder_name='resnet18', encoder_weights='imagenet'):\n",
    "        super().__init__()\n",
    "        base = smp.Unet(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None\n",
    "        )\n",
    "        self.encoder = base.encoder\n",
    "        self.decoder = base.decoder\n",
    "        \n",
    "        decoder_out_channels = base.segmentation_head[0].in_channels\n",
    "        \n",
    "        # Two heads\n",
    "        self.depth_head = nn.Conv2d(decoder_out_channels, 1, kernel_size=1)\n",
    "        self.seg_head = nn.Conv2d(decoder_out_channels, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)\n",
    "        dec = self.decoder(feats)\n",
    "        \n",
    "        depth = self.depth_head(dec)\n",
    "        seg = self.seg_head(dec)\n",
    "        \n",
    "        return depth, seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830febf-26d9-4454-b539-0f7b9cc1f33d",
   "metadata": {},
   "source": [
    "# Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c116f-739f-46c2-88b6-68dbd6d4529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your paths\n",
    "TRAIN_IMG_DIR = '../CamVid/train/'\n",
    "TRAIN_DEPTH_DIR = '../CamVid/train_labels/train_depths/'\n",
    "TRAIN_SEG_DIR = '../CamVid/train_labels/train_seg_npy/'\n",
    "VAL_IMG_DIR = '../CamVid/val/'\n",
    "VAL_DEPTH_DIR = '../CamVid/val_labels/val_depths/'\n",
    "VAL_SEG_DIR = '../CamVid/val_labels/val_seg_npy/'\n",
    "TEST_IMG_DIR = '../CamVid/test/'\n",
    "TEST_DEPTH_DIR = '../CamVid/test_labels/test_depths/'\n",
    "TEST_SEG_DIR = '../CamVid/test_labels/test_seg_npy/'\n",
    "CLASS_DICT_PATH = '../CamVid/class_dict.csv'\n",
    "\n",
    "# Define augmentation probabilities\n",
    "prob = {\n",
    "    'horizontal_flip': 0.5,\n",
    "    'color_jitter': 0.3\n",
    "}\n",
    "\n",
    "# Create DataModule\n",
    "data_module = DataModule(\n",
    "    train_img_dir=TRAIN_IMG_DIR,\n",
    "    train_depth_dir=TRAIN_DEPTH_DIR,\n",
    "    train_seg_dir=TRAIN_SEG_DIR,\n",
    "    val_img_dir=VAL_IMG_DIR,\n",
    "    val_depth_dir=VAL_DEPTH_DIR,\n",
    "    val_seg_dir=VAL_SEG_DIR,\n",
    "    test_img_dir=TEST_IMG_DIR,\n",
    "    test_depth_dir=TEST_DEPTH_DIR,\n",
    "    test_seg_dir=TEST_SEG_DIR,\n",
    "    batch_size=4,\n",
    "    num_workers=2,\n",
    "    prob=prob\n",
    ")\n",
    "\n",
    "# Test it\n",
    "data_module.setup()\n",
    "print(f\"Training samples: {len(data_module.train_dataset)}\")\n",
    "print(f\"Validation samples: {len(data_module.val_dataset)}\")\n",
    "print(f\"Test samples: {len(data_module.test_dataset)}\")\n",
    "\n",
    "sample_img, sample_depth, sample_mask = data_module.train_dataset[0]\n",
    "print(f\"\\nImage Size: {sample_img.shape}\")\n",
    "print(f\"Depth Map Size: {sample_depth.shape}\")\n",
    "print(f\"Segmentation Mask Size: {sample_mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f99324-cb27-47b1-95cf-71035c49f452",
   "metadata": {},
   "source": [
    "All shapes are 720x960, so the dataset is correctly loading the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536207a-1c5d-415b-9efb-5022534db99f",
   "metadata": {},
   "source": [
    "# MLFlow and Optuna Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb905cb4-4479-4fe4-9f81-c69b472b73ee",
   "metadata": {},
   "source": [
    "## MLFlow URI and Experiment Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42919fde-f0cd-413e-a78a-db59c9f16fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tracking URI\n",
    "mlflow.set_tracking_uri(\"mlruns\")\n",
    "\n",
    "# Set experiment name\n",
    "EXPERIMENT_NAME = \"depth_distillation_and_semantic_segmentation_optimization\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d085471-a2fe-4bd3-975b-55a1cee8fdfb",
   "metadata": {},
   "source": [
    "## Optuna Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a34431-3a10-499e-8fdd-616767b0f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization.\n",
    "    Trains depth distillation model and logs to MLflow.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        \n",
    "        # Training parameters\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 24, 32]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 5e-5, 1e-3),\n",
    "        'num_epochs': trial.suggest_int('num_epochs', 80, 200),\n",
    "        \n",
    "        # Augmentation probabilities\n",
    "        'prob_horizontal_flip': trial.suggest_float('prob_horizontal_flip', 0.0, 0.7),\n",
    "        'prob_color_jitter': trial.suggest_float('prob_color_jitter', 0.0, 0.7),\n",
    "        \n",
    "        # Optimizer parameters\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3),\n",
    "        \n",
    "        # Fixed parameters\n",
    "        'encoder_name': 'resnet18',\n",
    "        'encoder_weights': 'imagenet',\n",
    "        'num_workers': 2,\n",
    "        'gradient_clip_val': 1.0,\n",
    "    }\n",
    "    \n",
    "    # Start MLflow run (nested under parent run)\n",
    "    with mlflow.start_run(nested=True, run_name=f\"trial_{trial.number}\"):\n",
    "        \n",
    "        # Log all hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"trial_number\", trial.number)\n",
    "        \n",
    "        # Create augmentation probabilities dict\n",
    "        prob = {\n",
    "            'horizontal_flip': params['prob_horizontal_flip'],\n",
    "            'color_jitter': params['prob_color_jitter']\n",
    "        }\n",
    "        \n",
    "        # Create data module\n",
    "        data_module = DataModule(\n",
    "            train_img_dir=TRAIN_IMG_DIR,\n",
    "            train_depth_dir=TRAIN_DEPTH_DIR,\n",
    "            train_seg_dir=TRAIN_SEG_DIR,\n",
    "            val_img_dir=VAL_IMG_DIR,\n",
    "            val_depth_dir=VAL_DEPTH_DIR,\n",
    "            val_seg_dir=VAL_SEG_DIR,\n",
    "            test_img_dir=TEST_IMG_DIR,\n",
    "            test_depth_dir=TEST_DEPTH_DIR,\n",
    "            test_seg_dir=TEST_SEG_DIR,\n",
    "            batch_size=params['batch_size'],\n",
    "            num_workers=params['num_workers'],\n",
    "            prob=prob\n",
    "        )\n",
    "        data_module.setup()\n",
    "        \n",
    "        # Create student model\n",
    "        student = MultiTaskUnet(\n",
    "            num_classes=data_module.train_dataset.num_classes,\n",
    "            encoder_name=params['encoder_name'],\n",
    "            encoder_weights=params['encoder_weights'],\n",
    "        )\n",
    "        \n",
    "        num_params = sum(p.numel() for p in student.parameters())\n",
    "        mlflow.log_param(\"model_parameters\", num_params)\n",
    "        \n",
    "        # Create Lightning module\n",
    "        lightning_module = DepthSegDistillationModule(\n",
    "            student_model=student,\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Setup MLflow logger for this trial\n",
    "        mlflow_logger = MLFlowLogger(\n",
    "            experiment_name=EXPERIMENT_NAME,\n",
    "            run_id=mlflow.active_run().info.run_id\n",
    "        )\n",
    "        \n",
    "        # Setup callbacks\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_total_loss',\n",
    "            mode='min',\n",
    "            filename=f'trial_{trial.number}_best',\n",
    "            save_top_k=1,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_total_loss',\n",
    "            patience=15,\n",
    "            mode='min',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Optuna pruning callback\n",
    "        pruning_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_total_loss\")\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=params['num_epochs'],\n",
    "            logger=mlflow_logger,\n",
    "            callbacks=[checkpoint_callback, early_stopping, pruning_callback],\n",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            devices=1,\n",
    "            gradient_clip_val=params['gradient_clip_val'],\n",
    "            log_every_n_steps=10,\n",
    "            enable_progress_bar=False,\n",
    "            enable_model_summary=False\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        try:\n",
    "            trainer.fit(lightning_module, data_module)\n",
    "            \n",
    "            # FIXED: Get best validation loss properly with proper error handling\n",
    "            if checkpoint_callback.best_model_score is not None:\n",
    "                best_val_loss = float(checkpoint_callback.best_model_score)\n",
    "            else:\n",
    "                # Fallback: get from logged metrics\n",
    "                best_val_loss = float(trainer.callback_metrics.get('val_total_loss', float('inf')))\n",
    "            \n",
    "            # Log best metrics\n",
    "            mlflow.log_metric(\"best_val_total_loss\", best_val_loss)\n",
    "            mlflow.log_metric(\"epochs_trained\", trainer.current_epoch)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_results = trainer.test(\n",
    "                lightning_module, \n",
    "                data_module, \n",
    "                ckpt_path=checkpoint_callback.best_model_path\n",
    "            )\n",
    "            \n",
    "            if test_results:\n",
    "                test_loss = test_results[0].get('test_total_loss', None)\n",
    "                if test_loss:\n",
    "                    mlflow.log_metric(\"test_total_loss\", test_loss)\n",
    "            \n",
    "            # Log model artifact\n",
    "            mlflow.pytorch.log_model(student, \"model\")\n",
    "            \n",
    "            # Log checkpoint (only if path exists)\n",
    "            if checkpoint_callback.best_model_path:\n",
    "                mlflow.log_artifact(checkpoint_callback.best_model_path, \"checkpoints\")\n",
    "            \n",
    "            print(f\"Trial {trial.number}: val_total_loss={best_val_loss:.4f}, \"\n",
    "                  f\"epochs={trainer.current_epoch}\")\n",
    "            \n",
    "            return best_val_loss\n",
    "            \n",
    "        except optuna.TrialPruned:\n",
    "            print(f\"Trial {trial.number} pruned\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()  # Print full traceback for debugging\n",
    "            mlflow.log_param(\"status\", \"failed\")\n",
    "            mlflow.log_param(\"error\", str(e))\n",
    "            return float('inf')\n",
    "        finally:\n",
    "            # CRITICAL: Clean up resources after each trial\n",
    "            print(f\"Cleaning up Trial {trial.number}...\")\n",
    "            \n",
    "            # Delete objects\n",
    "            if 'trainer' in locals():\n",
    "                del trainer\n",
    "            if 'lightning_module' in locals():\n",
    "                del lightning_module\n",
    "            if 'student' in locals():\n",
    "                del student\n",
    "            if 'data_module' in locals():\n",
    "                del data_module\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            # Clear CUDA cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            print(f\"Trial {trial.number} cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa488dfe-0b3a-4591-834b-5757ce82af39",
   "metadata": {},
   "source": [
    "# Train Models -> Launch Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0cda5-cf4d-4bc6-b809-196353bfc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 50\n",
    "\n",
    "# Safety: end any active runs before starting\n",
    "mlflow.end_run()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Create parent MLflow run\n",
    "with mlflow.start_run(run_name=\"optuna_depth_distillation_and_semantic_seg\") as parent_run:\n",
    "    \n",
    "    # Log study configuration\n",
    "    mlflow.log_param(\"optimization_metric\", \"val_total_loss\")\n",
    "    mlflow.log_param(\"n_trials\", N_TRIALS)\n",
    "    mlflow.log_param(\"model_type\", \"Depth_Distillation_and_Semantic_Segmentation_UNet\")\n",
    "    mlflow.log_param(\"dataset\", \"CamVid\")\n",
    "    \n",
    "    # Create Optuna study\n",
    "    study = optuna.create_study(\n",
    "        study_name=\"depth_distillation_and_semantic_segmentation_optimization\",\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=15, n_warmup_steps=25),\n",
    "        storage=\"sqlite:///optuna_study_depth_and_seg_weighted.db\",  # Persist to database\n",
    "        load_if_exists=True  # Resume if interrupted\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"Starting Optuna optimization...\")\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Log best trial information\n",
    "    best_trial = study.best_trial\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best_trial.params.items()})\n",
    "    mlflow.log_metric(\"best_val_total_loss\", best_trial.value)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Optimization Complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best trial number: {best_trial.number}\")\n",
    "    print(f\"Best validation loss: {best_trial.value:.4f}\")\n",
    "    print(f\"Best hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Generate and log optimization visualizations\n",
    "    print(\"\\nGenerating optimization visualizations...\")\n",
    "    \n",
    "    try:\n",
    "        # Create plots directory\n",
    "        os.makedirs(\"plots\", exist_ok=True)\n",
    "        \n",
    "        # Plot optimization history\n",
    "        fig1 = viz.plot_optimization_history(study)\n",
    "        fig1.write_html(\"plots/optimization_history.html\")\n",
    "        mlflow.log_artifact(\"plots/optimization_history.html\", \"optimization_plots\")\n",
    "        \n",
    "        # Plot parameter importances\n",
    "        fig2 = viz.plot_param_importances(study)\n",
    "        fig2.write_html(\"plots/param_importances.html\")\n",
    "        mlflow.log_artifact(\"plots/param_importances.html\", \"optimization_plots\")\n",
    "        \n",
    "        # Plot parallel coordinate\n",
    "        fig3 = viz.plot_parallel_coordinate(study)\n",
    "        fig3.write_html(\"plots/parallel_coordinate.html\")\n",
    "        mlflow.log_artifact(\"plots/parallel_coordinate.html\", \"optimization_plots\")\n",
    "        \n",
    "        # Plot slice\n",
    "        fig4 = viz.plot_slice(study)\n",
    "        fig4.write_html(\"plots/slice_plot.html\")\n",
    "        mlflow.log_artifact(\"plots/slice_plot.html\", \"optimization_plots\")\n",
    "        \n",
    "        print(\"Optimization plots logged to MLflow\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not generate visualizations: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a51fd0-e97c-4049-ac2b-ad341e9e5d7e",
   "metadata": {},
   "source": [
    "# Evaluate Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269179bb-4590-4735-9f10-27e0c3c7f695",
   "metadata": {},
   "source": [
    "## Load Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a12172d-18f6-4071-a276-d12067ac9f47",
   "metadata": {},
   "source": [
    "### Identify Best Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e37b4-e10c-492f-acd9-7dfdb20378b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Set experiment name\n",
    "EXPERIMENT_NAME = \"depth_distillation_and_semantic_segmentation_optimization\"\n",
    "\n",
    "# Get experiment\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if experiment is None:\n",
    "    raise ValueError(f\"Experiment '{EXPERIMENT_NAME}' not found!\")\n",
    "\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Search for best run\n",
    "all_runs = client.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"metrics.best_val_total_loss > 0\",\n",
    "    order_by=[\"metrics.best_val_total_loss ASC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "if len(all_runs) == 0:\n",
    "    raise ValueError(\"No runs found with best_val_total_loss metric!\")\n",
    "\n",
    "best_run = all_runs[0]\n",
    "best_run_id = best_run.info.run_id\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"Best Run Found!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Run ID: {best_run_id}\")\n",
    "print(f\"Best Val Loss: {best_run.data.metrics.get('best_val_total_loss', 'N/A'):.4f}\")\n",
    "\n",
    "try:\n",
    "    # Download artifacts directory\n",
    "    artifacts_path = client.download_artifacts(best_run_id, \"\")\n",
    "    print(f\"\\nDownloaded artifacts to: {artifacts_path}\")\n",
    "    \n",
    "    # Look for ckpt model in checkpoints\n",
    "    checkpoints_dir = os.path.join(artifacts_path, \"checkpoints\")\n",
    "    \n",
    "    if os.path.exists(checkpoints_dir):\n",
    "        # Find ckpt file\n",
    "        trial_number = best_run.data.params.get('trial_number', '0')\n",
    "        ckpt_files = [f for f in os.listdir(checkpoints_dir) \n",
    "                       if f.endswith('.ckpt')]\n",
    "        \n",
    "        if ckpt_files:\n",
    "            best_checkpoint_path = os.path.join(checkpoints_dir, ckpt_files[0])\n",
    "            print(f\"Loading model from: {best_checkpoint_path}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No ckpt files found in {checkpoints_dir}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"checkpoints directory not found at {artifacts_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from artifacts: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238626a-1cf7-4843-ab24-a1a8d9e18ec7",
   "metadata": {},
   "source": [
    "### Evaluate over Validation Set\n",
    "We load and test the model over validation set to assure everithing is right and we get the same \"best_val_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f847a-b1ac-43c0-ba06-16cad861eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters from MLflow - FIXED\n",
    "best_params = {\n",
    "    'encoder_name': best_run.data.params['encoder_name'],\n",
    "    'encoder_weights': best_run.data.params['encoder_weights'],\n",
    "    'learning_rate': float(best_run.data.params['learning_rate']),\n",
    "    'weight_decay': float(best_run.data.params['weight_decay']),\n",
    "    'batch_size': int(best_run.data.params['batch_size'])\n",
    "}\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create test data module\n",
    "test_data_module = DataModule(\n",
    "    train_img_dir=TRAIN_IMG_DIR,\n",
    "    train_depth_dir=TRAIN_DEPTH_DIR,\n",
    "    train_seg_dir=TRAIN_SEG_DIR,\n",
    "    val_img_dir=VAL_IMG_DIR,\n",
    "    val_depth_dir=VAL_DEPTH_DIR,\n",
    "    val_seg_dir=VAL_SEG_DIR,\n",
    "    test_img_dir=VAL_IMG_DIR, # We use val split\n",
    "    test_depth_dir=VAL_DEPTH_DIR, # We use val split\n",
    "    test_seg_dir=VAL_SEG_DIR, # We use val split\n",
    "    batch_size=best_params['batch_size'],\n",
    "    num_workers=2,\n",
    "    prob=None\n",
    ")\n",
    "test_data_module.setup()\n",
    "\n",
    "# Create the model with best architecture\n",
    "student = MultiTaskUnet(\n",
    "    num_classes=test_data_module.train_dataset.num_classes,\n",
    "    encoder_name=best_params['encoder_name'],\n",
    "    encoder_weights=best_params['encoder_weights'],\n",
    ")\n",
    "\n",
    "# Create Lightning module\n",
    "lightning_module = DepthSegDistillationModule(\n",
    "    student_model=student,\n",
    "    lr=best_params['learning_rate'],\n",
    "    weight_decay=best_params['weight_decay']\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(best_checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lightning_module.load_state_dict(checkpoint['state_dict'])\n",
    "lightning_module.eval()\n",
    "\n",
    "print(\"\\nBest model loaded successfully!\")\n",
    "\n",
    "# Create trainer for testing\n",
    "test_trainer = pl.Trainer(\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    logger=False\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Best Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_results = test_trainer.test(lightning_module, test_data_module)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in test_results[0].items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d2d4e-b085-4a95-a9b1-7671933f9f0e",
   "metadata": {},
   "source": [
    "Perfect. We got the same loss, so the model was found and loaded correctly.\n",
    "Now, lets evaluate over the Test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb78e3-c9c3-4c12-bd1c-591126258134",
   "metadata": {},
   "source": [
    "### Evaluate over Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488df58a-cfc5-484d-9f35-43efd3136115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters from MLflow - FIXED\n",
    "best_params = {\n",
    "    'encoder_name': best_run.data.params['encoder_name'],\n",
    "    'encoder_weights': best_run.data.params['encoder_weights'],\n",
    "    'learning_rate': float(best_run.data.params['learning_rate']),\n",
    "    'weight_decay': float(best_run.data.params['weight_decay']),\n",
    "    'batch_size': int(best_run.data.params['batch_size'])\n",
    "}\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create test data module\n",
    "test_data_module = DataModule(\n",
    "    train_img_dir=TRAIN_IMG_DIR,\n",
    "    train_depth_dir=TRAIN_DEPTH_DIR,\n",
    "    train_seg_dir=TRAIN_SEG_DIR,\n",
    "    val_img_dir=VAL_IMG_DIR,\n",
    "    val_depth_dir=VAL_DEPTH_DIR,\n",
    "    val_seg_dir=VAL_SEG_DIR,\n",
    "    test_img_dir=TEST_IMG_DIR,\n",
    "    test_depth_dir=TEST_DEPTH_DIR,\n",
    "    test_seg_dir=TEST_SEG_DIR,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    num_workers=2,\n",
    "    prob=None\n",
    ")\n",
    "test_data_module.setup()\n",
    "\n",
    "# Create the model with best architecture\n",
    "student = MultiTaskUnet(\n",
    "    num_classes=test_data_module.train_dataset.num_classes,\n",
    "    encoder_name=best_params['encoder_name'],\n",
    "    encoder_weights=best_params['encoder_weights'],\n",
    ")\n",
    "\n",
    "# Create Lightning module\n",
    "lightning_module = DepthSegDistillationModule(\n",
    "    student_model=student,\n",
    "    lr=best_params['learning_rate'],\n",
    "    weight_decay=best_params['weight_decay']\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(best_checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lightning_module.load_state_dict(checkpoint['state_dict'])\n",
    "lightning_module.eval()\n",
    "\n",
    "print(\"\\nBest model loaded successfully!\")\n",
    "\n",
    "# Create trainer for testing\n",
    "test_trainer = pl.Trainer(\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    logger=False\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Best Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_results = test_trainer.test(lightning_module, test_data_module)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in test_results[0].items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7eb5fd-0ef2-46ad-9755-d5341de88e42",
   "metadata": {},
   "source": [
    "## Visualize Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ec084-c044-49aa-ba80-4392a4651a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, num_samples=5, seed=42, title_prefix=\"Test\"):\n",
    "    \"\"\"Visualize student predictions vs teacher depth maps and segmentation\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    random.seed(seed)\n",
    "    offset = random.randrange(0, len(dataset) - num_samples)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_samples, len(dataset))):\n",
    "            # Create figure with gridspec\n",
    "            fig = plt.figure(figsize=(15, 10))\n",
    "            gs = gridspec.GridSpec(2, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            image, teacher_depth, seg_mask_gt = dataset[offset + i]\n",
    "            \n",
    "            # Get student predictions\n",
    "            student_depth_pred, student_seg_pred = model(image.unsqueeze(0).to(device))\n",
    "            student_depth_pred = student_depth_pred.squeeze().cpu().numpy()\n",
    "            student_seg_pred = torch.argmax(student_seg_pred, dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Denormalize image for visualization\n",
    "            img_display = image.clone()\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            img_display = img_display * std + mean\n",
    "            img_display = torch.clamp(img_display, 0, 1)\n",
    "            \n",
    "            # [0, 0] and [1, 0]: Original Image (spans 2 rows)\n",
    "            ax_img = fig.add_subplot(gs[:, 0])\n",
    "            ax_img.imshow(img_display.permute(1, 2, 0))\n",
    "            ax_img.set_title(f'{title_prefix} Sample {i+1}: Input Image', fontsize=12, fontweight='bold')\n",
    "            ax_img.axis('off')\n",
    "            \n",
    "            # [0, 1]: Teacher Depth Map\n",
    "            ax_teacher_depth = fig.add_subplot(gs[0, 1])\n",
    "            im1 = ax_teacher_depth.imshow(teacher_depth.numpy(), cmap='Spectral')\n",
    "            ax_teacher_depth.set_title('Teacher Depth (DA3)', fontsize=12, fontweight='bold')\n",
    "            ax_teacher_depth.axis('off')\n",
    "            plt.colorbar(im1, ax=ax_teacher_depth, fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # [0, 2]: Ground Truth Semantic Segmentation\n",
    "            ax_seg_gt = fig.add_subplot(gs[0, 2])\n",
    "            im2 = ax_seg_gt.imshow(seg_mask_gt.numpy(), cmap='tab20')\n",
    "            ax_seg_gt.set_title('GT Semantic Segmentation', fontsize=12, fontweight='bold')\n",
    "            ax_seg_gt.axis('off')\n",
    "            plt.colorbar(im2, ax=ax_seg_gt, fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # [1, 1]: Student Predicted Depth Map\n",
    "            ax_student_depth = fig.add_subplot(gs[1, 1])\n",
    "            im3 = ax_student_depth.imshow(student_depth_pred, cmap='Spectral')\n",
    "            ax_student_depth.set_title('Student Depth (U-Net ResNet18)', fontsize=12, fontweight='bold')\n",
    "            ax_student_depth.axis('off')\n",
    "            plt.colorbar(im3, ax=ax_student_depth, fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # [1, 2]: Student Predicted Semantic Segmentation\n",
    "            ax_student_seg = fig.add_subplot(gs[1, 2])\n",
    "            im4 = ax_student_seg.imshow(student_seg_pred, cmap='tab20')\n",
    "            ax_student_seg.set_title('Student Semantic Segmentation', fontsize=12, fontweight='bold')\n",
    "            ax_student_seg.axis('off')\n",
    "            plt.colorbar(im4, ax=ax_student_seg, fraction=0.046, pad=0.04)\n",
    "            \n",
    "            plt.savefig(f'{title_prefix.lower()}_predictions_sample_{i+1}.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "print(\"Visualizing predictions on TEST set:\")\n",
    "test_dataset = DepthDistillationDataset(\n",
    "    img_dir=TEST_IMG_DIR,\n",
    "    depth_dir=TEST_DEPTH_DIR,\n",
    "    seg_dir=TEST_SEG_DIR,\n",
    "    num_classes=32,\n",
    "    is_train=False\n",
    ")\n",
    "visualize_predictions(lightning_module.student, test_dataset, num_samples=5, title_prefix=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb14595-33c1-4f6d-8ff8-73953117ac81",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a30c4e-3459-4dca-a1ea-2e641b197c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, dataloader, split_name=\"Test\"):\n",
    "    \"\"\"Calculate depth estimation metrics\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_mse = 0\n",
    "    total_mae = 0\n",
    "    total_abs_rel = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, teacher_depth, seg_mask_gt in dataloader:\n",
    "            images = images.to(device)\n",
    "            teacher_depth = teacher_depth.to(device)\n",
    "            seg_mask_gt = seg_mask_gt.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            student_depth_pred, student_seg_pred = model(images)\n",
    "            student_depth_pred = student_depth_pred.squeeze(1)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = F.mse_loss(student_depth_pred, teacher_depth)\n",
    "            mae = F.l1_loss(student_depth_pred, teacher_depth)\n",
    "            abs_rel = torch.mean(torch.abs(student_depth_pred - teacher_depth) / (teacher_depth + 1e-8))\n",
    "            \n",
    "            total_mse += mse.item() * images.size(0)\n",
    "            total_mae += mae.item() * images.size(0)\n",
    "            total_abs_rel += abs_rel.item() * images.size(0)\n",
    "            num_samples += images.size(0)\n",
    "    \n",
    "    metrics = {\n",
    "        'MSE': total_mse / num_samples,\n",
    "        'MAE': total_mae / num_samples,\n",
    "        'RMSE': np.sqrt(total_mse / num_samples),\n",
    "        'Abs Rel': total_abs_rel / num_samples\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{split_name} Set Metrics:\")\n",
    "    print(\"=\" * 40)\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name:12s}: {value:.4f}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Create data module\n",
    "test_data_module = DataModule(\n",
    "    train_img_dir=TRAIN_IMG_DIR,\n",
    "    train_depth_dir=TRAIN_DEPTH_DIR,\n",
    "    train_seg_dir=TRAIN_SEG_DIR,\n",
    "    val_img_dir=VAL_IMG_DIR,\n",
    "    val_depth_dir=VAL_DEPTH_DIR,\n",
    "    val_seg_dir=VAL_SEG_DIR,\n",
    "    test_img_dir=TEST_IMG_DIR,\n",
    "    test_depth_dir=TEST_DEPTH_DIR,\n",
    "    test_seg_dir=TEST_SEG_DIR,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    num_workers=2,\n",
    "    prob=None\n",
    ")\n",
    "test_data_module.setup(stage='test')\n",
    "\n",
    "# Calculate metrics on TEST set\n",
    "test_loader = test_data_module.test_dataloader()\n",
    "test_metrics = calculate_metrics(lightning_module.student, test_loader, split_name=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189361ab-2d97-4376-8297-adb424ac7b4d",
   "metadata": {},
   "source": [
    "## Test Set Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a534e2b-c202-41f2-85c2-77f462e6658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_test_analysis(model, dataset, num_samples=10):\n",
    "    \"\"\"Perform detailed analysis on test samples\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    errors = []\n",
    "\n",
    "    worst_sample_idx = -1\n",
    "    best_sample_idx = -1\n",
    "    best_mse = np.inf\n",
    "    worst_mse = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_samples, len(dataset))):\n",
    "            image, teacher_depth, seg_mask_gt = dataset[i]\n",
    "            \n",
    "            # Get prediction\n",
    "            student_depth_pred, student_seg_pred = model(image.unsqueeze(0).to(device))\n",
    "            student_depth_pred = student_depth_pred.squeeze().cpu()\n",
    "            \n",
    "            # Calculate per-sample error\n",
    "            mse = F.mse_loss(student_depth_pred, teacher_depth).item()\n",
    "            mae = F.l1_loss(student_depth_pred, teacher_depth).item()\n",
    "            \n",
    "            errors.append({\n",
    "                'sample_idx': i,\n",
    "                'MSE': mse,\n",
    "                'MAE': mae\n",
    "            })\n",
    "\n",
    "            # Track worst and best samples taking into account MSE value\n",
    "            if mse > worst_mse:\n",
    "                worst_mse = mse\n",
    "                worst_sample_idx = i\n",
    "                \n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_sample_idx = i\n",
    "    \n",
    "    # Print per-sample results\n",
    "    print(\"\\nPer-Sample Test Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for error in errors:\n",
    "        print(f\"Sample {error['sample_idx']:3d}: MSE={error['MSE']:.4f}, MAE={error['MAE']:.4f}\")\n",
    "    \n",
    "    # Statistical summary\n",
    "    mse_values = [e['MSE'] for e in errors]\n",
    "    mae_values = [e['MAE'] for e in errors]\n",
    "    \n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"MSE - Mean: {np.mean(mse_values):.4f}, Std: {np.std(mse_values):.4f}\")\n",
    "    print(f\"MAE - Mean: {np.mean(mae_values):.4f}, Std: {np.std(mae_values):.4f}\")\n",
    "\n",
    "    return worst_sample_idx, best_sample_idx\n",
    "\n",
    "def visualize_edge_predictions(model, dataset, worst_sample_idx, best_sample_idx):\n",
    "    \"\"\"Visualize student predictions vs teacher depth maps\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Worst predicted sample\n",
    "        image, teacher_depth, seg_mask_gt = dataset[worst_sample_idx]\n",
    "            \n",
    "        # Get student prediction\n",
    "        student_depth_pred, student_seg_pred = model(image.unsqueeze(0).to(device))\n",
    "        student_pred = student_depth_pred.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Denormalize image for visualization\n",
    "        img_display = image.clone()\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        img_display = img_display * std + mean\n",
    "        img_display = torch.clamp(img_display, 0, 1)\n",
    "        \n",
    "        # Plot\n",
    "        axes[0, 0].imshow(img_display.permute(1, 2, 0))\n",
    "        axes[0, 0].set_title(f'Worst predicted Test sample')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(teacher_depth.numpy(), cmap='Spectral')\n",
    "        axes[0, 1].set_title('Teacher Depth (DA3)')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(student_pred, cmap='Spectral')\n",
    "        axes[0, 2].set_title('Student Depth (U-Net ResNet18)')\n",
    "        axes[0, 2].axis('off')\n",
    "\n",
    "        # Best predicted sample\n",
    "        image, teacher_depth, seg_mask_gt = dataset[best_sample_idx]\n",
    "            \n",
    "        # Get student prediction\n",
    "        student_depth_pred, student_seg_pred = model(image.unsqueeze(0).to(device))\n",
    "        student_pred = student_depth_pred.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Denormalize image for visualization\n",
    "        img_display = image.clone()\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        img_display = img_display * std + mean\n",
    "        img_display = torch.clamp(img_display, 0, 1)\n",
    "        \n",
    "        # Plot\n",
    "        axes[1, 0].imshow(img_display.permute(1, 2, 0))\n",
    "        axes[1, 0].set_title(f'Best predicted Test sample')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(teacher_depth.numpy(), cmap='Spectral')\n",
    "        axes[1, 1].set_title('Teacher Depth (DA3)')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(student_pred, cmap='Spectral')\n",
    "        axes[1, 2].set_title('Student Depth (U-Net ResNet18)')\n",
    "        axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'worst_best_test_depth_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run detailed analysis\n",
    "test_dataset = DepthDistillationDataset(\n",
    "    img_dir=TEST_IMG_DIR,\n",
    "    depth_dir=TEST_DEPTH_DIR,\n",
    "    seg_dir=TEST_SEG_DIR,\n",
    ")\n",
    "\n",
    "worst_sample_idx, best_sample_idx = detailed_test_analysis(lightning_module.student, test_dataset, num_samples=len(test_dataset))\n",
    "print(f\"Worst predicted sample found at image #{worst_sample_idx}\")\n",
    "print(f\"Best predicted sample found at image #{best_sample_idx}\")\n",
    "print()\n",
    "\n",
    "visualize_edge_predictions(lightning_module.student, test_dataset, worst_sample_idx, best_sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78117667-9286-422f-ad88-41cc56a1f792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
